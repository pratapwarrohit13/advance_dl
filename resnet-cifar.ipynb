{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T07:46:43.789025Z","iopub.execute_input":"2021-07-30T07:46:43.789463Z","iopub.status.idle":"2021-07-30T07:46:43.799155Z","shell.execute_reply.started":"2021-07-30T07:46:43.789401Z","shell.execute_reply":"2021-07-30T07:46:43.797560Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation\nfrom tensorflow.keras.layers import AveragePooling2D, Input\nfrom tensorflow.keras.layers import Flatten, add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport os\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:43.801551Z","iopub.execute_input":"2021-07-30T07:46:43.802352Z","iopub.status.idle":"2021-07-30T07:46:43.816628Z","shell.execute_reply.started":"2021-07-30T07:46:43.802304Z","shell.execute_reply":"2021-07-30T07:46:43.815385Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"batch_size = 32 # orig paper trained all networks with batch_size=128\nepochs = 20\ndata_augmentation = True\nnum_classes = 10\n\n# subtracting pixel mean improves accuracy\nsubtract_pixel_mean = True","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:43.819853Z","iopub.execute_input":"2021-07-30T07:46:43.820492Z","iopub.status.idle":"2021-07-30T07:46:43.828959Z","shell.execute_reply.started":"2021-07-30T07:46:43.820448Z","shell.execute_reply":"2021-07-30T07:46:43.827866Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"n = 3\nversion = 1\n\nif version == 1:\n    depth = n * 6 + 2\nelif version == 2:\n    depth = n * 9 + 2","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:43.831360Z","iopub.execute_input":"2021-07-30T07:46:43.831881Z","iopub.status.idle":"2021-07-30T07:46:43.840661Z","shell.execute_reply.started":"2021-07-30T07:46:43.831834Z","shell.execute_reply":"2021-07-30T07:46:43.839540Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_type = 'ResNet%dv%d' % (depth, version)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:43.842537Z","iopub.execute_input":"2021-07-30T07:46:43.843128Z","iopub.status.idle":"2021-07-30T07:46:43.854553Z","shell.execute_reply.started":"2021-07-30T07:46:43.843083Z","shell.execute_reply":"2021-07-30T07:46:43.853376Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"(features_train, target_train), (features_test, target_test) = cifar10.load_data()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:43.857854Z","iopub.execute_input":"2021-07-30T07:46:43.858278Z","iopub.status.idle":"2021-07-30T07:46:44.749092Z","shell.execute_reply.started":"2021-07-30T07:46:43.858217Z","shell.execute_reply":"2021-07-30T07:46:44.747946Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"input_shape = features_train.shape[1:]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:44.750772Z","iopub.execute_input":"2021-07-30T07:46:44.751239Z","iopub.status.idle":"2021-07-30T07:46:44.756118Z","shell.execute_reply.started":"2021-07-30T07:46:44.751170Z","shell.execute_reply":"2021-07-30T07:46:44.754924Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"features_train = features_train.astype('float32') / 255\nfeatures_test = features_test.astype('float32') / 255","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:44.757935Z","iopub.execute_input":"2021-07-30T07:46:44.758805Z","iopub.status.idle":"2021-07-30T07:46:45.105838Z","shell.execute_reply.started":"2021-07-30T07:46:44.758759Z","shell.execute_reply":"2021-07-30T07:46:45.104716Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"if subtract_pixel_mean:\n    features_train_mean = np.mean(features_train, axis=0)\n    features_train -= features_train_mean\n    features_test -= features_train_mean\n\nprint('x_train shape:', features_train.shape)\nprint(features_train.shape[0], 'train samples')\nprint(features_test.shape[0], 'test samples')\nprint('y_train shape:', target_train.shape)\n\n# convert class vectors to binary class matrices.\ntarget_train = to_categorical(target_train, num_classes)\ntarget_test = to_categorical(target_test, num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.110145Z","iopub.execute_input":"2021-07-30T07:46:45.110488Z","iopub.status.idle":"2021-07-30T07:46:45.338192Z","shell.execute_reply.started":"2021-07-30T07:46:45.110456Z","shell.execute_reply":"2021-07-30T07:46:45.336777Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\ny_train shape: (50000, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    lr = 1e-3\n    if epoch > 180:\n        lr *= 0.5e-3\n    elif epoch > 160:\n        lr *= 1e-3\n    elif epoch > 120:\n        lr *= 1e-2\n    elif epoch > 80:\n        lr *= 1e-1\n    print('Learning rate: ', lr)\n    return lr","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.340775Z","iopub.execute_input":"2021-07-30T07:46:45.341286Z","iopub.status.idle":"2021-07-30T07:46:45.350604Z","shell.execute_reply.started":"2021-07-30T07:46:45.341219Z","shell.execute_reply":"2021-07-30T07:46:45.348881Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def resnet_layer(inputs,\n                 num_filters=16,\n                 kernel_size=3,\n                 strides=1,\n                 activation='relu',\n                 batch_normalization=True,\n                 conv_first=True):\n   \n    conv = Conv2D(num_filters,\n                  kernel_size=kernel_size,\n                  strides=strides,\n                  padding='same',\n                  kernel_initializer='he_normal',\n                  kernel_regularizer=l2(1e-4))\n\n    x = inputs\n    if conv_first:\n        x = conv(x)\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n    else:\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n        x = conv(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.352511Z","iopub.execute_input":"2021-07-30T07:46:45.353311Z","iopub.status.idle":"2021-07-30T07:46:45.364012Z","shell.execute_reply.started":"2021-07-30T07:46:45.353266Z","shell.execute_reply":"2021-07-30T07:46:45.362706Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def resnet_v1(input_shape, depth, num_classes=10):\n\n    if (depth - 2) % 6 != 0:\n        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n    # start model definition.\n    num_filters = 16\n    num_res_blocks = int((depth - 2) / 6)\n\n    inputs = Input(shape=input_shape)\n    x = resnet_layer(inputs=inputs)\n    # instantiate the stack of residual units\n    for stack in range(3):\n        for res_block in range(num_res_blocks):\n            strides = 1\n            # first layer but not first stack\n            if stack > 0 and res_block == 0:  \n                strides = 2  # downsample\n            y = resnet_layer(inputs=x,\n                             num_filters=num_filters,\n                             strides=strides)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters,\n                             activation=None)\n            # first layer but not first stack\n            if stack > 0 and res_block == 0:\n                # linear projection residual shortcut\n                # connection to match changed dims\n                x = resnet_layer(inputs=x,\n                                 num_filters=num_filters,\n                                 kernel_size=1,\n                                 strides=strides,\n                                 activation=None,\n                                 batch_normalization=False)\n            x = add([x, y])\n            x = Activation('relu')(x)\n        num_filters *= 2\n\n    # add classifier on top.\n    # v1 does not use BN after last shortcut connection-ReLU\n    x = AveragePooling2D(pool_size=8)(x)\n    y = Flatten()(x)\n    outputs = Dense(num_classes,\n                    activation='softmax',\n                    kernel_initializer='he_normal')(y)\n\n    # instantiate model.\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.366311Z","iopub.execute_input":"2021-07-30T07:46:45.366626Z","iopub.status.idle":"2021-07-30T07:46:45.382386Z","shell.execute_reply.started":"2021-07-30T07:46:45.366597Z","shell.execute_reply":"2021-07-30T07:46:45.381257Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def resnet_v2(input_shape, depth, num_classes=10):\n    \n    if (depth - 2) % 9 != 0:\n        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n    # start model definition.\n    num_filters_in = 16\n    num_res_blocks = int((depth - 2) / 9)\n\n    inputs = Input(shape=input_shape)\n    # v2 performs Conv2D with BN-ReLU\n    # on input before splitting into 2 paths\n    x = resnet_layer(inputs=inputs,\n                     num_filters=num_filters_in,\n                     conv_first=True)\n\n    # instantiate the stack of residual units\n    for stage in range(3):\n        for res_block in range(num_res_blocks):\n            activation = 'relu'\n            batch_normalization = True\n            strides = 1\n            if stage == 0:\n                num_filters_out = num_filters_in * 4\n                # first layer and first stage\n                if res_block == 0:  \n                    activation = None\n                    batch_normalization = False\n            else:\n                num_filters_out = num_filters_in * 2\n                # first layer but not first stage\n                if res_block == 0:\n                    # downsample\n                    strides = 2 \n\n            # bottleneck residual unit\n            y = resnet_layer(inputs=x,\n                             num_filters=num_filters_in,\n                             kernel_size=1,\n                             strides=strides,\n                             activation=activation,\n                             batch_normalization=batch_normalization,\n                             conv_first=False)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters_in,\n                             conv_first=False)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters_out,\n                             kernel_size=1,\n                             conv_first=False)\n            if res_block == 0:\n                # linear projection residual shortcut connection\n                # to match changed dims\n                x = resnet_layer(inputs=x,\n                                 num_filters=num_filters_out,\n                                 kernel_size=1,\n                                 strides=strides,\n                                 activation=None,\n                                 batch_normalization=False)\n            x = add([x, y])\n\n        num_filters_in = num_filters_out\n\n    # add classifier on top.\n    # v2 has BN-ReLU before Pooling\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = AveragePooling2D(pool_size=8)(x)\n    y = Flatten()(x)\n    outputs = Dense(num_classes,\n                    activation='softmax',\n                    kernel_initializer='he_normal')(y)\n\n    # instantiate model.\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.386011Z","iopub.execute_input":"2021-07-30T07:46:45.386817Z","iopub.status.idle":"2021-07-30T07:46:45.403041Z","shell.execute_reply.started":"2021-07-30T07:46:45.386768Z","shell.execute_reply":"2021-07-30T07:46:45.401609Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if version == 2:\n    model = resnet_v2(input_shape=input_shape, depth=depth)\nelse:\n    model = resnet_v1(input_shape=input_shape, depth=depth)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=lr_schedule(0)),\n              metrics=['acc'])\nmodel.summary()\n\n\nprint(model_type)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.404505Z","iopub.execute_input":"2021-07-30T07:46:45.405121Z","iopub.status.idle":"2021-07-30T07:46:45.967934Z","shell.execute_reply.started":"2021-07-30T07:46:45.405075Z","shell.execute_reply":"2021-07-30T07:46:45.966808Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Learning rate:  0.001\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 32, 32, 16)   64          conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nactivation_19 (Activation)      (None, 32, 32, 16)   0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 32, 32, 16)   2320        activation_19[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 32, 32, 16)   64          conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nactivation_20 (Activation)      (None, 32, 32, 16)   0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 32, 32, 16)   2320        activation_20[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 32, 32, 16)   64          conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 32, 32, 16)   0           activation_19[0][0]              \n                                                                 batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nactivation_21 (Activation)      (None, 32, 32, 16)   0           add_9[0][0]                      \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 32, 32, 16)   2320        activation_21[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 32, 32, 16)   64          conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nactivation_22 (Activation)      (None, 32, 32, 16)   0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 32, 32, 16)   2320        activation_22[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 32, 32, 16)   64          conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nadd_10 (Add)                    (None, 32, 32, 16)   0           activation_21[0][0]              \n                                                                 batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nactivation_23 (Activation)      (None, 32, 32, 16)   0           add_10[0][0]                     \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 32, 32, 16)   2320        activation_23[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 32, 32, 16)   64          conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nactivation_24 (Activation)      (None, 32, 32, 16)   0           batch_normalization_24[0][0]     \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 32, 32, 16)   2320        activation_24[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 32, 32, 16)   64          conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nadd_11 (Add)                    (None, 32, 32, 16)   0           activation_23[0][0]              \n                                                                 batch_normalization_25[0][0]     \n__________________________________________________________________________________________________\nactivation_25 (Activation)      (None, 32, 32, 16)   0           add_11[0][0]                     \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 16, 16, 32)   4640        activation_25[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nactivation_26 (Activation)      (None, 16, 16, 32)   0           batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 16, 16, 32)   544         activation_25[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nadd_12 (Add)                    (None, 16, 16, 32)   0           conv2d_30[0][0]                  \n                                                                 batch_normalization_27[0][0]     \n__________________________________________________________________________________________________\nactivation_27 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 16, 16, 32)   9248        activation_27[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nactivation_28 (Activation)      (None, 16, 16, 32)   0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 16, 16, 32)   9248        activation_28[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nadd_13 (Add)                    (None, 16, 16, 32)   0           activation_27[0][0]              \n                                                                 batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nactivation_29 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        activation_29[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n__________________________________________________________________________________________________\nactivation_30 (Activation)      (None, 16, 16, 32)   0           batch_normalization_30[0][0]     \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 16, 16, 32)   9248        activation_30[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nadd_14 (Add)                    (None, 16, 16, 32)   0           activation_29[0][0]              \n                                                                 batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\nactivation_31 (Activation)      (None, 16, 16, 32)   0           add_14[0][0]                     \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 8, 8, 64)     18496       activation_31[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n__________________________________________________________________________________________________\nactivation_32 (Activation)      (None, 8, 8, 64)     0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 8, 8, 64)     2112        activation_31[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nadd_15 (Add)                    (None, 8, 8, 64)     0           conv2d_37[0][0]                  \n                                                                 batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nactivation_33 (Activation)      (None, 8, 8, 64)     0           add_15[0][0]                     \n__________________________________________________________________________________________________\nconv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n__________________________________________________________________________________________________\nactivation_34 (Activation)      (None, 8, 8, 64)     0           batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\nconv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n__________________________________________________________________________________________________\nadd_16 (Add)                    (None, 8, 8, 64)     0           activation_33[0][0]              \n                                                                 batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nactivation_35 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n__________________________________________________________________________________________________\nconv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n__________________________________________________________________________________________________\nactivation_36 (Activation)      (None, 8, 8, 64)     0           batch_normalization_36[0][0]     \n__________________________________________________________________________________________________\nconv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_36[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n__________________________________________________________________________________________________\nadd_17 (Add)                    (None, 8, 8, 64)     0           activation_35[0][0]              \n                                                                 batch_normalization_37[0][0]     \n__________________________________________________________________________________________________\nactivation_37 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n__________________________________________________________________________________________________\naverage_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_37[0][0]              \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n==================================================================================================\nTotal params: 274,442\nTrainable params: 273,066\nNon-trainable params: 1,376\n__________________________________________________________________________________________________\nResNet20v1\n","output_type":"stream"}]},{"cell_type":"code","source":"save_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nfilepath = os.path.join(save_dir, model_name)\n\n# prepare callbacks for model saving and for learning rate adjustment.\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_acc',\n                             verbose=1,\n                             save_best_only=True)\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               min_lr=0.5e-6)\n\ncallbacks = [checkpoint, lr_reducer, lr_scheduler]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.970572Z","iopub.execute_input":"2021-07-30T07:46:45.971031Z","iopub.status.idle":"2021-07-30T07:46:45.982932Z","shell.execute_reply.started":"2021-07-30T07:46:45.970987Z","shell.execute_reply":"2021-07-30T07:46:45.981657Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"if not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(features_train, target_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(features_test, target_test),\n              shuffle=True,\n              callbacks=callbacks)\nelse:\n    print('Using real-time data augmentation.')\n    # this will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        # set input mean to 0 over the dataset\n        featurewise_center=False,\n        # set each sample mean to 0\n        samplewise_center=False,\n        # divide inputs by std of dataset\n        featurewise_std_normalization=False,\n        # divide each input by its std\n        samplewise_std_normalization=False,\n        # apply ZCA whitening\n        zca_whitening=False,\n        # randomly rotate images in the range (deg 0 to 180)\n        rotation_range=0,\n        # randomly shift images horizontally\n        width_shift_range=0.1,\n        # randomly shift images vertically\n        height_shift_range=0.1,\n        # randomly flip images\n        horizontal_flip=True,\n        # randomly flip images\n        vertical_flip=False)\n\n    # compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(features_train)\n\n    steps_per_epoch =  math.ceil(len(features_train) / batch_size)\n    # fit the model on the batches generated by datagen.flow().\n    model.fit(x=datagen.flow(features_train, target_train, batch_size=batch_size),\n              verbose=1,\n              epochs=epochs,\n              validation_data=(features_test, target_test),\n              steps_per_epoch=steps_per_epoch,\n              callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:46:45.984744Z","iopub.execute_input":"2021-07-30T07:46:45.985274Z","iopub.status.idle":"2021-07-30T08:04:53.606158Z","shell.execute_reply.started":"2021-07-30T07:46:45.985197Z","shell.execute_reply":"2021-07-30T08:04:53.605098Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Using real-time data augmentation.\nEpoch 1/20\nLearning rate:  0.001\n1563/1563 [==============================] - 59s 35ms/step - loss: 1.9941 - acc: 0.3847 - val_loss: 1.3529 - val_acc: 0.5613\n\nEpoch 00001: val_acc improved from -inf to 0.56130, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.001.h5\nEpoch 2/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 34ms/step - loss: 1.2445 - acc: 0.6137 - val_loss: 2.0334 - val_acc: 0.4781\n\nEpoch 00002: val_acc did not improve from 0.56130\nEpoch 3/20\nLearning rate:  0.001\n1563/1563 [==============================] - 55s 35ms/step - loss: 1.0465 - acc: 0.6865 - val_loss: 1.0924 - val_acc: 0.6829\n\nEpoch 00003: val_acc improved from 0.56130 to 0.68290, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.003.h5\nEpoch 4/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 34ms/step - loss: 0.9385 - acc: 0.7256 - val_loss: 1.2163 - val_acc: 0.6682\n\nEpoch 00004: val_acc did not improve from 0.68290\nEpoch 5/20\nLearning rate:  0.001\n1563/1563 [==============================] - 55s 35ms/step - loss: 0.8729 - acc: 0.7547 - val_loss: 1.0596 - val_acc: 0.6967\n\nEpoch 00005: val_acc improved from 0.68290 to 0.69670, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.005.h5\nEpoch 6/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 35ms/step - loss: 0.8098 - acc: 0.7804 - val_loss: 0.8970 - val_acc: 0.7487\n\nEpoch 00006: val_acc improved from 0.69670 to 0.74870, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.006.h5\nEpoch 7/20\nLearning rate:  0.001\n1563/1563 [==============================] - 53s 34ms/step - loss: 0.7777 - acc: 0.7891 - val_loss: 1.0703 - val_acc: 0.7032\n\nEpoch 00007: val_acc did not improve from 0.74870\nEpoch 8/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 35ms/step - loss: 0.7412 - acc: 0.8028 - val_loss: 1.0781 - val_acc: 0.7156\n\nEpoch 00008: val_acc did not improve from 0.74870\nEpoch 9/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 34ms/step - loss: 0.7335 - acc: 0.8077 - val_loss: 0.9964 - val_acc: 0.7354\n\nEpoch 00009: val_acc did not improve from 0.74870\nEpoch 10/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 35ms/step - loss: 0.7088 - acc: 0.8141 - val_loss: 1.1769 - val_acc: 0.7058\n\nEpoch 00010: val_acc did not improve from 0.74870\nEpoch 11/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 34ms/step - loss: 0.6888 - acc: 0.8241 - val_loss: 0.7891 - val_acc: 0.7988\n\nEpoch 00011: val_acc improved from 0.74870 to 0.79880, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.011.h5\nEpoch 12/20\nLearning rate:  0.001\n1563/1563 [==============================] - 53s 34ms/step - loss: 0.6674 - acc: 0.8348 - val_loss: 0.9555 - val_acc: 0.7598\n\nEpoch 00012: val_acc did not improve from 0.79880\nEpoch 13/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 34ms/step - loss: 0.6545 - acc: 0.8388 - val_loss: 0.7945 - val_acc: 0.7943\n\nEpoch 00013: val_acc did not improve from 0.79880\nEpoch 14/20\nLearning rate:  0.001\n1563/1563 [==============================] - 54s 35ms/step - loss: 0.6432 - acc: 0.8407 - val_loss: 0.9528 - val_acc: 0.7580\n\nEpoch 00014: val_acc did not improve from 0.79880\nEpoch 15/20\nLearning rate:  0.001\n1563/1563 [==============================] - 53s 34ms/step - loss: 0.6341 - acc: 0.8445 - val_loss: 0.8926 - val_acc: 0.7730\n\nEpoch 00015: val_acc did not improve from 0.79880\nEpoch 16/20\nLearning rate:  0.001\n1563/1563 [==============================] - 55s 35ms/step - loss: 0.6208 - acc: 0.8502 - val_loss: 0.8222 - val_acc: 0.7920\n\nEpoch 00016: val_acc did not improve from 0.79880\nEpoch 17/20\nLearning rate:  0.001\n1563/1563 [==============================] - 55s 35ms/step - loss: 0.6162 - acc: 0.8485 - val_loss: 0.7422 - val_acc: 0.8144\n\nEpoch 00017: val_acc improved from 0.79880 to 0.81440, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.017.h5\nEpoch 18/20\nLearning rate:  0.001\n1563/1563 [==============================] - 53s 34ms/step - loss: 0.6112 - acc: 0.8541 - val_loss: 0.6811 - val_acc: 0.8353\n\nEpoch 00018: val_acc improved from 0.81440 to 0.83530, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.018.h5\nEpoch 19/20\nLearning rate:  0.001\n1563/1563 [==============================] - 55s 35ms/step - loss: 0.5949 - acc: 0.8628 - val_loss: 1.3161 - val_acc: 0.6947\n\nEpoch 00019: val_acc did not improve from 0.83530\nEpoch 20/20\nLearning rate:  0.001\n1563/1563 [==============================] - 53s 34ms/step - loss: 0.6002 - acc: 0.8593 - val_loss: 0.6823 - val_acc: 0.8358\n\nEpoch 00020: val_acc improved from 0.83530 to 0.83580, saving model to /kaggle/working/saved_models/cifar10_ResNet20v1_model.020.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = model.evaluate(features_test,\n                        target_test,\n                        batch_size=batch_size,\n                        verbose=0)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:04:53.608197Z","iopub.execute_input":"2021-07-30T08:04:53.608671Z","iopub.status.idle":"2021-07-30T08:04:55.919223Z","shell.execute_reply.started":"2021-07-30T08:04:53.608611Z","shell.execute_reply":"2021-07-30T08:04:55.914957Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Test loss: 0.6823045015335083\nTest accuracy: 0.8357999920845032\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}